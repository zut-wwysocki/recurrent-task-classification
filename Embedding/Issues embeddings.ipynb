{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353ab937",
   "metadata": {},
   "source": [
    "# Process issues for classification w.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c1135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 3.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.0a0+936e930)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.15.0a0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.6.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 136.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 110.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "\u001b[K     |████████████████████████████████| 330 kB 145.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[K     |████████████████████████████████| 168 kB 137.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 106.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 87.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (9.0.1)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=9bbc12e0ec297803fc524613f3eaf5cd3cc5bf6c869bb8d745e3391c93e49c6c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nbfefhde/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: fsspec, filelock, huggingface-hub, tokenizers, safetensors, transformers, sentencepiece, nltk, sentence-transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.1 nltk-3.8.1 safetensors-0.4.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.15.0 transformers-4.36.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f03c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f88811d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356fb711",
   "metadata": {},
   "outputs": [],
   "source": [
    "Issues = {\n",
    "     'P1': 'P1-g4-c',\n",
    "     'P2': 'P2-g4-c',\n",
    "     'P3': 'P3-g4-c',\n",
    "     'P4': 'P4-g4-c',\n",
    "     'P5': 'P5-g4-c',\n",
    "     'P6': 'P6-g4-c',\n",
    "}\n",
    "\n",
    "\n",
    "datasets ={\n",
    "    'Issues': Issues, \n",
    "}\n",
    "\n",
    "datasets_path = \"../Data\" ## DATA NOT PUBLISHED DUE TO CONFIDENTIONALITY RESTRICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe465c",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39adf14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2ba48e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedder\n",
    "\n",
    "class Embedder:\n",
    "    \n",
    "    def __init__(self, model_id, max_tokens=512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModel.from_pretrained(model_id).to(device)\n",
    "    \n",
    "    def _pooling(self, outputs: torch.Tensor, inputs: Dict,  strategy: str = 'cls') -> np.ndarray:\n",
    "        if strategy == 'cls':\n",
    "            outputs = outputs[:, 0]\n",
    "        elif strategy == 'mean':\n",
    "            outputs = torch.sum(\n",
    "                outputs * inputs[\"attention_mask\"][:, :, None], dim=1) / torch.sum(inputs[\"attention_mask\"])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return outputs.detach().cpu().numpy()\n",
    "    \n",
    "\n",
    "    def embed(self, doc):\n",
    "        inputs = self.tokenizer(doc, max_length=self.max_tokens, padding=True, truncation=True, \n",
    "                                    stride=64, return_overflowing_tokens=True,\n",
    "                                   return_tensors='pt')\n",
    "        del inputs['overflow_to_sample_mapping']\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.cuda()\n",
    "        outputs = self.model(**inputs).last_hidden_state\n",
    "        embeddings = self._pooling(outputs, inputs, 'cls')\n",
    "        return embeddings.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "20d73d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_id = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "max_tokens = 512\n",
    "embedder = Embedder(model_id, max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d1648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = {}\n",
    "anonym = 0\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    dataset = datasets[dataset_name]\n",
    "    for project, repo in list(dataset.items()):\n",
    "        print(f\"{project}\")\n",
    "    \n",
    "        data_path = f\"{datasets_path}/{repo}.csv\"\n",
    "        print(data_path)\n",
    "        \n",
    "        data = pd.read_csv(data_path, sep='$')\n",
    "        emb_title = np.array([embedder.embed(x).reshape(-1) for x in data[\"title\"]])\n",
    "        emb_desc = np.array([embedder.embed(x).reshape(-1) for x in data[\"description\"].fillna('')])\n",
    "        features = np.concatenate((emb_title, emb_desc), axis=1)\n",
    "        df = pd.DataFrame(features);\n",
    "        df = df.add_prefix(\"emb\")\n",
    "        df[\"id\"] = data[\"id\"]\n",
    "        df[\"class_name\"] = data[\"class_name\"]\n",
    "        df[\"class_value\"] = data[\"class_value\"]\n",
    "        df[\"type_value\"] = data[\"type_value\"]\n",
    "        df[\"creator\"] = data[\"creator\"]\n",
    "        df[\"assignee\"] = data[\"assignee\"]\n",
    "        if anonym:\n",
    "            df[\"contents\"] = data[\"id\"].astype(str) + \":\" + data[\"type_name\"] \\\n",
    "            + \":\" + data[\"effort\"].astype(str) + \"h\" \n",
    "        else:\n",
    "            df[\"contents\"] = data[\"id\"].astype(str) + \":\" + data[\"type_name\"] \\\n",
    "            + \":\" + data[\"effort\"].astype(str) + \"h \" + data[\"users\"] + \":\" \\\n",
    "            + \" - \" + data[\"title\"] + \" - \" + data[\"description\"].fillna('')\n",
    "            \n",
    "        dummies = pd.get_dummies(df['type_value'], prefix=\"type\")\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df.drop(['type_value'], inplace=True, axis=1)\n",
    "        \n",
    "        \n",
    "        out_data_path = f\"../Data/{repo}-emb.csv\"\n",
    "        print(out_data_path)\n",
    "        df.to_csv(out_data_path, sep='$', index=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf226a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
